{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft and Experiment Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Report on \"Learning internal Representations by Error Propagation\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The link to this report:  \n",
    "<https://github.com/SnowWindSaveYou/UTS_ML2019_99151300/blob/master/A1_Report.ipynb>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the information technology rapidly grows in recent years, the neural network technology becomes a popular research direction, the more and more neural network-based applications have been created and intergraded into our lives. As I also want to research at that neural network direction, I have the interest at how is it works, and that paper “learning internal representations by error propagation” is one important bedrock of this technology, so I choose to reading to get more understanding of the principle of neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This research proposed a new kind of algorithm called generalized delta rule, also known as the back-propagation algorithm. This algorithm is a kind of chain rule with the gradient descent method. The goal of this algorithm is to achieve a reliable way of adjusting the hidden layer of a multi-layer network, allow it can find an appropriate internal representation can provide a good mapping from the input to the output patterns. \n",
    "\n",
    "In order to achieve their goal, the basic idea is making the output of this network close to the desired output, so this algorithm aims to minimize the error between them. In this paper uses the square of the difference between the actual output and the desired output as the metric of error. On this premise, they swap the parameter and the variable of a single layout function, make the variable as parameter and the weight(parameter) of this layout as the variable to make a cost function, then the negative gradient of this cost function is the direction can descent it to find an appropriate weight of this layout which can get the smaller cost result. \n",
    "This method can work on each layout with the layout before it, the parameter is the result of the last layout, and the variable is the weight of current layout, by repeat doing this step form the last layout of the network can propagate the error of the last layout to the total network, and this process is a composite function can derivate by chain rule. In this case, use the batch of sample input and corresponding desired output of different required output, the algorithm can use the sum of their error to propagate the change to the total network to adjust the appropriate weight for each layout.\n",
    "\n",
    "In addition to the main content of generalized delta rule, this paper also proposed many additional things to implement this method. One is that paper proposed to use the semi-linear activation functions such as the logistic activation in each hidden layout to solve the problem of linear network no advantage in the non-linear problem, and by using it also can minimize the calculation cost. And second, it also proposed to use a learning rate to accelerate the speed of gradient descent.  After those, this paper also points out some weakness of this algorithm such as the local optimum and symmetry breaking and provide some solution of those problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Innovation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the time of this work, people had proposed two layout network using perception for mapping the similar input patterns to similar output patterns, and this method can works on the pattern never present before,  but people also found that simple two-layer network have the limitation on mapping complex pattern. However, they did recognize that the potential power of multi-layer network with internal representations, that multi-layout perception can fitting in any of the mapping problems if they are large enough and have an appropriate connection between each node. However, there was a big lack of how to efficient find this appropriate connection, although there were some basic responses to this lack, but they were not developed yet.\n",
    "\n",
    "In this case, the generalized delta rule has a great innovation of providing a generalized method of finding the appropriate connection weight in the multi hidden layout. Because of it, now this method is already widely used in most of the industry neural network application and becomes one of the important bedrock of neural network technology. And more detailly, one of main innovation for algorithm self is it brings the back-propagation method and chain rule to the application, those reduce the complexity of the learning method of multi layout network, make it calculable.\n",
    "\n",
    "And the other main innovation of this paper is it solved the challenge of none linear problem. The challenge of it is the perception in the hidden layout is a linear method, which means the multi-layout network assembles by those linear perceptions is also linear, while the linear method will have the limitation on the none linear problem. In this case, this paper provides an innovation of using activation function in each layout of perception to make the function becomes semi-linear, then that semi-linear function allows the algorithm can solve the none linear problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, this paper shows a very high technical quality by mathematic logics, experiment examples and well explanation structure. Their explanation not only let the reader can easier understand his research, and also force them to believe the potentiality of the multi-layout network and their algorithm.\n",
    " \n",
    "At the beginning of the paper, the authors drew their aims by prove the benefits and the challenge of the multi-layout network. They first introduced the use of two layout associative networks and their limitation and compare with the multi-layout network, then use the example of XOR problem and the reference of other research to prove the importance of analysing it. Then, the authors of this paper throw out the challenge in the multi-layout network, clearly shows their aim and the motivation of the research and arouse reader’s interesting.\n",
    " \n",
    "After, authors use mathematic logic to explains their proposed algorithm step by step. First, they use the detail mathematical derivation of the linear version of their algorithm and summarize what those mathematical function dose to make reader can have a deep understanding of the linear version of their algorism. And then they explain the use of activation function to popularize this algorithm to the none linear problems with other mathematical derivation and the summary to make the reader going to the next level of understanding their algorithm.\n",
    " \n",
    "After authors finish explains their algorithm, they challenged many problems such as the XOR problem, parity problem, symmetry problem and T-C problem shows the power and the application of their algorithm and the multi-layout network to make the reader believe the potentiality of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application and X-factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generalized delta rule is already widely used in many applications now, but even if we don’t look at the current achievement of this algorithm, it is still can see a wide range of application prospects. The main application of this algorithm is to provide the ability of the multi-layout network can learn an appropriate weight by itself with target data.  And because of the technology of the multi-layout network self has the ability of the simulation the mapping between input and output pattern if they have appropriate weight, so the corroboration of them is able to achieve an almighty powerful pattern mapping model without the pre-known knowledge and the other human-designed functions. In this case, the multi-layout network with generalized delta rule is suitable to solve the problems have a high dimension or have high complexity such as the image, sound, and language process because the human is hard to find an appropriate feature to make a generalized rule in those kind of problem . \n",
    " \n",
    "The future development of this algorithm can focus on fix the problems of it, such as the problem of vanishing and exploding gradients, and lack of theory leads it requires to explore the learning rate and other hyper-parameter.(Ma, Lewis & Kleijn 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall strucutre is clear. I found reading is easy / difficult. The paper could have been more attractive if the authors had organised ... / provided ... "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "Rumelhart, D.E., Hinton, G.E. & Williams, R.J. 1985, Learning internal representations by error propagation, California Univ San Diego La Jolla Inst for Cognitive Science.\n",
    "\n",
    "Kurenkov, A. 2015, 'A ‘brief’history of neural nets and deep learning', andreykurenkov. com, December, vol. 24.\n",
    "\n",
    "Ma, W.-D.K., Lewis, J. & Kleijn, W.B. 2019, 'The HSIC Bottleneck: Deep Learning without Back-Propagation', arXiv preprint arXiv:1908.01580.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
