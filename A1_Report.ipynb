{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Report on \"Learning internal Representations by Error Propagation\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The link to this report:  \n",
    "<https://github.com/SnowWindSaveYou/UTS_ML2019_99151300/blob/master/A1_Report.ipynb>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the information technology rapidly grows in recent years, the neural network technology becomes a popular research direction, the more and more neural network-based applications have been created and intergraded into our lives. As I also want to research at that neural network direction, I have the interest at how is it works, and that paper “learning internal representations by error propagation” is one important bedrock of this technology, so I choose to reading to get more understanding of the principle of neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This research proposed a new kind of algorithm called generalized delta rule, also known as the back-propagation algorithm. This algorithm is a kind of chain rule with the gradient descent method. The goal of this algorithm is to achieve a reliable way of adjusting the hidden layer of a multi-layer network, allow it can find an appropriate internal representation can provide a good mapping from the input to the output patterns. \n",
    "\n",
    "In order to achieve their goal, the basic idea is making the output of this network close to the desired output, so this algorithm aims to minimize the error between them. In this paper uses the square of the difference between the actual output and the desired output as the metric of error. On this premise, they swap the parameter and the variable of a single layout function, make the variable as parameter and the weight(parameter) of this layout as the variable to make a cost function, then the negative gradient of this cost function is the direction can descent it to find an appropriate weight of this layout which can get the smaller cost result. \n",
    "This method can work on each layout with the layout before it, the parameter is the result of the last layout, and the variable is the weight of current layout, by repeat doing this step form the last layout of the network can propagate the error of the last layout to the total network, and this process is a composite function can derivate by chain rule. In this case, use the batch of sample input and corresponding desired output of different required output, the algorithm can use the sum of their error to propagate the change to the total network to adjust the appropriate weight for each layout.\n",
    "\n",
    "In addition to the main content of generalized delta rule, this paper also proposed many additional things to improve this method. One is that paper proposed to use the semi-linear activation functions such as the logistic activation in each hidden layout to solve the problem of linear network no advantage in the non-linear problem, and by using it also can minimize the calculation cost. And second, it also proposed to use a learning rate to accelerate the speed of gradient descent.  \n",
    "\n",
    "After those, this paper also points out some weakness of this algorithm, such as the local optimum and symmetry breaking and provide some solution to those problems. For the local optimum problem, they point out it doesn't have too much impact through experience. And for symmetry breaking problem, they proposed to use small random weight to counteract this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Innovation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the time of this work, people had proposed two layout network using perception for mapping the similar input patterns to similar output patterns, and this method can works on the pattern never present before,  but people also found that simple two-layer network have the limitation on mapping complex pattern. However, they did recognize that the potential power of multi-layer network with internal representations, that multi-layout perception can fitting in any of the mapping problems if they are large enough and have an appropriate connection between each node. However, there was a big lack of how to efficient find this appropriate connection, although there were some basic responses to this lack, but they were not developed yet.\n",
    "\n",
    "In this case, the generalized delta rule has a great contribution of providing a generalized method of finding the appropriate connection weight in the multi hidden layout. Because of it, now this method is already widely used in most of the industry neural network application and becomes one of the important bedrock of neural network technology. And more detailly, one of main innovation for algorithm self is it brings the back-propagation method and chain rule to the application, those reduce the complexity of the learning method of multi layout network, make it calculable.\n",
    "\n",
    "And the other main contribution of this paper is it solved the challenge of none linear problem. The challenge of it is the perception in the hidden layout is a linear method, which means the multi-layout network assembles by those linear perceptions is also linear, while the linear method will have the limitation on the none linear problem. In this case, this paper provides an innovation of using activation function in each layout of perception to make the function becomes semi-linear, then that semi-linear function allows the algorithm can solve the none linear problems.\n",
    "\n",
    "Furthermore, they are not only have proposed algorithm self, but they also provide many useful experiences through deep analysis of this algorithm by experiments. Such as in theory, the gradient decline does not necessarily find the global minimum, it more likely to find a local minimum, but after numerous experiments they point out that local minimum is not important, the network still got high accuracy even it is in the local minimum; And they found that if system starts at a kind of local maximum it will maximising the derivation result which will lead the error never return, so authors analysis the principle of it and proposed to using random weight to counteract that symmetry breaking problem; and after tried many learning rates authors also provide a hyper-parameter of paradigmatic leaning rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical quality"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, this paper has a very high technical quality by using the reliable reference, mathematic logics, and experiment examples, the authors use them clearly and deeply explain the generalized delta rule and shows the power of it with the multi-layout network.\n",
    "\n",
    "At the beginning of the paper, the authors drew their aim by proving the benefits and the challenge on the multi-layout network. They first introduced the use of two layout associative networks, and then compare it with the multi-layout network by demonstrating the example of XOR problem to show its limitation and the benefits of the multi-layout network. Then, the authors of this paper throw out the challenge of adjusting weight in the multi-layout network, clearly and strongly shows their aim, motivation and importance of the research and arouse reader’s interesting.\n",
    "\n",
    "Then they explain the generalized delta rule by mathematical logic and clear structure. They split that generalized delta rule into many tasks with purpose, and for each task shows the detail derivation while summarizing the principle and the reason for them in general langrage. This method of explanation not only provide the strong evidence of the generalized delta rule, and also makes the reader can easier understand what the principle is it.\n",
    "\n",
    "After explaining the theory, authors use numerous experiments to support the power of generalized delta rule. The experiments they are used include XOR problem, parity problem, symmetry problem and T-C problem, those examples show the ability of the generalized delta rule can generalize on most non-linear problems, and for each experiment they all recorded the detailed result, challenge they meet, and their solution, provides a powerful evidence of the power of multi-layout network with their algorithm, also pointed out the potential direction of future research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application and X-factor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generalized delta rule is already widely used in many applications now, but even if we don’t look at the current achievement of this algorithm, it is still can see a wide range of application prospects. The main application of this algorithm is to provide the ability of the multi-layout network can learn an appropriate weight by itself with target data.  And because of the technology of the multi-layout network self has the ability of the simulation the mapping between input and output pattern if they have appropriate weight, so the corroboration of them is able to achieve an almighty powerful pattern mapping model without the pre-known knowledge and the other human-designed functions. In this case, the multi-layout network with generalized delta rule is suitable to solve the problems have a high dimension or have high complexity such as the image, sound, and language process because the human is hard to find an appropriate feature to make a generalized rule in those kind of problem . \n",
    "\n",
    "Although this algorithm is useful, but it still has many problems such as the local minima, the problem of vanishing and exploding gradients, and lack of theory leads it requires to explore the learning rate and other hyper-parameter. (Ma, Lewis & Kleijn 2019) so one way of the future development can concentrate on fix those problems, such as the study of Xavier initialization method proposed to reduce the exploding gradients. The other way of find a way can make the algorithem learning more effecient, such as using calculated feature as input, and using converlution to compress input feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall structure is clear. I found reading is easy. It firstly provides the reliable context of the research let me can easily get the propose of its research, and when it explains the troublesome mathematic derivation it also provides vividly sentence summarize the reason and the effect of each stage in the algorithm at same time, it makes me can efferently understand what those function doing without detailly analysis the functions. Furthermore, the experiment examples used in the paper have provides a sense of reliable makes people can believe the potential power of this algorithm. if there is anything that can be improve, i think is when authors explain the feedforward can use some diagrams rather than pure mathmatical functions. By reading this paper and the related materials, I got the deeper understanding of the principle of the neural network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "Rumelhart, D.E., Hinton, G.E. & Williams, R.J. 1985, Learning internal representations by error propagation, California Univ San Diego La Jolla Inst for Cognitive Science.\n",
    "\n",
    "Kurenkov, A. 2015, 'A ‘brief’history of neural nets and deep learning', andreykurenkov. com, December, vol. 24.\n",
    "\n",
    "Ma, W.-D.K., Lewis, J. & Kleijn, W.B. 2019, 'The HSIC Bottleneck: Deep Learning without Back-Propagation', arXiv preprint arXiv:1908.01580.\n",
    "\n",
    "Zhang, C., Bengio, S., Hardt, M., Recht, B. & Vinyals, O. 2016, 'Understanding deep learning requires rethinking generalization', arXiv preprint arXiv:1611.03530.\n",
    "\n",
    "Glorot, X. & Bengio, Y. 2010, 'Understanding the difficulty of training deep feedforward neural networks', pp. 249-56."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
